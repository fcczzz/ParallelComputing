\documentclass{ctexart}
\usepackage{babel}
\usepackage{anyfontsize}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{floatrow}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{dirtree}
\definecolor{teal}{RGB}{0,128,128}
\setlist[itemize]{noitemsep}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{codered}{rgb}{0.89,0.4,.45}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{codered},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\title{\textbf{并行计算大作业课程报告}}
\author{方驰正 PB21000163}

\begin{document}
\begin{sloppypar}

\maketitle

% \tableofcontents
% \newpage

% \ctexset{abstractname=摘要}
% \begin{abstract}

% \end{abstract}

\section{实验目的}
在本次实验中，我们使用MNIST手写数字数据集，训练了一个mlp自动感知机，并使用其预测了测试集，将结果提交至kaggle上的手写数字识别项目。分别实现了cpu串行版本和gpu并行版本，并对gpu并行版本进行优化，最终达到了96.1\%的准确率，以及相比于cpu版本的9.5倍的加速比。

\section{实验环境}
\begin{itemize}
    \item 操作系统：WSL2 Ubuntu 22.04
    \item 编程语言：C++, cuda
    \item 编译器：g++ 11.4.0
    \item cuda版本：12.4
    \item GPU：NVIDIA GeForce RTX 4060 Laptop
\end{itemize}

\section{算法设计和实现}
以下为主要的文件目录结构：
\dirtree{%
.1 {/} .
.2 {/data} .
.3 {train.csv} .
.3 {test.csv} .
.2 {/output} .
.3 {submission.csv} .
.2 {main.cpp} .
.2 {mnist.h} . 
.2 {mnist.cpp} .
.2 {matrix.h} .
.2 {matrix.cpp} .
.2 {Makefile} .
}
可以看到，核心代码主要分为三个部分，分别是mnist.h和mnist.cpp，matrix.h和matrix.cpp，以及main.cpp。其中mnist.h和mnist.cpp主要负责数据的读取和预处理，matrix.h和matrix.cpp主要负责矩阵运算，main.cpp主要负责模型的训练和预测。下面我们分别介绍这三个部分的实现。

\subsection{mnist}
这一部分为数据的读取和预处理部分。我们使用了mnist数据集，其中train.csv为训练集，test.csv为测试集。

具体地，在mnist.h中的声明如下：
\begin{lstlisting}[language=C++]
struct mnist_data {
    unsigned char label;
    unsigned char a[28 * 28];
};
std::vector<mnist_data> input(std::string filename, bool is_train);
void output(std::string filename, std::vector<int> data);
\end{lstlisting}

可以看到，我们定义了一个mnist\_data结构体，其中包含了一个label和一个28*28的数组，表示该数据的标签和特征。input函数用于读取数据，output函数用于输出数据。
\subsection{matrix}
\begin{lstlisting}[language=C++]
struct Mat {
    int n, m;
    double *a;
    double &operator()(int i, int j) const;
    void random_init(int n, int m, double loc, double scale); //随机初始化
    void zero_init(int n, int m);
    Mat();
    Mat(Mat &&_);
    Mat(const Mat &_);
    Mat operator=(Mat &&_);
    Mat operator=(const Mat &_);
    ~Mat();
    Mat operator*(const Mat &_) const;
    Mat operator*(const double &_) const;
    Mat operator+(const Mat &_) const;
    Mat operator-(const Mat &_) const;
    Mat relu() const;                                         //激活函数
    Mat relu_() const;                                        //激活函数的导数
    Mat softmax() const;                                      // softmax函数
    Mat softmax_() const;                                     // softmax函数的导数
    Mat T() const;                                            //转置
    double sum() const;

    Mat mult(const Mat &_) const; //矩阵对应元素相乘
};

double Loss(const Mat &y, const Mat &y_hat);
double Accuracy(const Mat &y, const Mat &y_hat);

\end{lstlisting}
在这里，我们定义了一个Mat结构体，用来表示一个矩阵。同时，我们定义了一系列在mlp训练与预测中需要用到的函数，如矩阵乘法、矩阵加法、矩阵减法、激活函数、softmax函数、损失函数、准确率等。
\subsection{mlp}
这是整个模型的核心部分，我们使用了一个三层的mlp模型，分别为输入层、隐藏层和输出层。其中，输入层包含784个神经元，隐藏层包含256个神经元，输出层包含10个神经元。我们使用了relu作为激活函数，使用softmax作为输出层的激活函数。同时，我们使用梯度下降方法进行反向传播。在训练时，我们使用了交叉熵作为损失函数。主要相关代码如下：
\begin{lstlisting}[language=C++]
struct MLP {
    Mat W1, W2, b1, b2;
    double lr;

    void forward(const Mat &input, Mat &z1, Mat &a1,
                 Mat &z2, Mat &a2) {
        z1 = input * W1 + b1;
        a1 = z1.relu();
        z2 = a1 * W2 + b2;
        a2 = z2.softmax();
    }

    void backward(Mat &input, Mat &z1, Mat &a1, Mat &z2,
                  Mat &a2, Mat &label) {
        Mat delta2 = (a2 - label).mult(z2.softmax_());
        Mat delta1 = (delta2 * W2.T()).mult(z1.relu_());

        Mat dW2 = a1.T() * delta2;
        Mat dW1 = input.T() * delta1;

        W1 = W1 - dW1 * lr;
        W2 = W2 - dW2 * lr;
        b1 = b1 - delta1 * lr;
        b2 = b2 - delta2 * lr;
    }

    std::pair<double, double>
    step(Mat &input, Mat &label,
         bool train = true) { 
        Mat z1, a1, z2, a2;
        forward(input, z1, a1, z2, a2);

        double loss = Loss(a2, label);
        double accuracy = Accuracy(a2, label);

        if (train) backward(input, z1, a1, z2, a2, label);
        return std::make_pair(loss, accuracy);
    }
} net;
\end{lstlisting}
可以看到，我们定义了一个MLP结构体，其中包含了W1、W2、b1、b2和lr等参数。forward函数用于前向传播，backward函数用于反向传播，step函数用于一次训练迭代。在训练时，我们使用了随机梯度下降法。
\begin{lstlisting}[language=C++]
void train(std::vector<mnist_data> &data, int epoch) {
    net.init(784, 256, 10, 0.01);
    ans = net;
    double best_accuracy = 0;
    double t = clock();
    for (int i = 0; i < epoch; i++) {
        std::vector<double> loss, accuracy;
        int t = 0;
        for (auto &d : data) {
            Mat input, label;
            input.zero_init(1, 784);
            for (int i = 0; i < 784; i++)
                input.a[i] = d.a[i] / 255.0;

            label.zero_init(1, 10);
            label.a[d.label] = 1;
            auto res = net.step(input, label);
            loss.push_back(res.first);
            accuracy.push_back(res.second);
        }
        double average_accuracy =
            std::accumulate(accuracy.begin(),
                            accuracy.end(), 0.0)
            / accuracy.size();
        if (average_accuracy > best_accuracy) {
            best_accuracy = average_accuracy;
            ans = net;
        }
    }
}

std::vector<int> test(std::vector<mnist_data> &data) {
    std::vector<int> res;
    for (auto &d : data) {
        Mat input;
        input.zero_init(1, 784);
        for (int i = 0; i < 784; i++)
            input.a[i] = d.a[i] / 255.0;

        Mat z1, a1, z2, a2;
        ans.forward(input, z1, a1, z2, a2);

        int predict = 0;
        for (int i = 0; i < 10; i++) {
            if (a2.a[i] > a2.a[predict]) predict = i;
        }
        res.push_back(predict);
    }
    return res;
}
\end{lstlisting}
如上为训练以及测试部分的代码。

可以看到，在训练时，我们将数据归一化到[0, 1]之间，并进行若干个epoch的训练。取平均准确率最高的模型作为最终的模型。为了简单起见，我们在测试时没有使用交叉验证，而是直接使用了训练集的数据进行测试。

在测试时，我们将数据归一化到[0, 1]之间，然后使用训练好的模型进行预测。最终将预测结果输出到submission.csv文件中。

\subsection{串行实验结果}
编译并运行后，输出如下：
\begin{lstlisting}
epoch:0 average_acc:0.861167 time used:128.734s
epoch:1 average_acc:0.925143 time used:257.458s
epoch:2 average_acc:0.941952 time used:384.517s
epoch:3 average_acc:0.952071 time used:510.324s
epoch:4 average_acc:0.959524 time used:635.946s
epoch:5 average_acc:0.964476 time used:770.660s
epoch:6 average_acc:0.968905 time used:899.398s
epoch:7 average_acc:0.972143 time used:1024.27s
epoch:8 average_acc:0.975429 time used:1150.56s
epoch:9 average_acc:0.978310 time used:1279.45s
\end{lstlisting}
可以看到，经过10个epoch的训练，最终的准确率为97.8\%，尚未过拟合。但是训练时间较长，达到了23分钟。将输出的测试数据提交至kaggle上，得到的准确率如下：
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{cpu_result.jpg}
    \caption{串行版本的kaggle提交结果}
\end{figure}
由于训练时间较长，我们尝试并行计算。
\section{并行实现和优化}
可以看到，串行版本的程序的运行时间还是比较长，因此我们尝试使用cuda，在gpu上进行并行计算。
由于主要的计算部分都在matrix.cpp中，因此我们重点的优化部分也在这里。这里，我们以矩阵乘法为例，介绍我们的优化方法。
\subsection{编写核函数}
\begin{lstlisting}[language=C++]
void Mat::zero_init(int N, int M) {
    n = N, m = M;
    a = new double[n * m];
    memset(a, 0, sizeof(double) * n * m);
}
Mat::~Mat() {
    if (a) delete[] a;
}
Mat Mat::operator*(const double &_) const {
    Mat res;
    res.zero_init(n, m);
    for (int i = 0; i < n; i++)
        for (int j = 0; j < m; j++)
            res(i, j) = (*this)(i, j) * _;
    return res;
}
\end{lstlisting}
这是最原始的矩阵乘法的实现，我们可以看到，我们首先在栈空间上申请了一个新的矩阵res，对其进行矩阵乘法运算，返回后调用析构函数释放内存。我们使用cuda优化后的代码如下：
\begin{lstlisting}[language=C++]
void Mat::zero_init(int N, int M) {
    n = N, m = M;
    cudaMalloc(&a, sizeof(double) * n * m);
    cudaMemset(a, 0, sizeof(double) * n * m);
}
Mat::~Mat() {
    if (a) cudaFree(a);
}
__global__ void mult_mat_kernel(double *a, double *b,
                                double *c, int n, int m,
                                int k) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n * k) {
        int x = i / k, y = i % k;
        c[i] = 0;
        for (int j = 0; j < m; j++)
            c[i] += a[x * m + j] * b[j * k + y];
    }
}
Mat Mat::operator*(const Mat &_) const {
    assert(m == _.n);
    Mat res;
    res.zero_init(n, _.m);
    mult_mat_kernel<<<(n * _.m + 1023) / 1024, 1024>>>(
        a, _.a, res.a, n, m, _.m);
    return res;
}
\end{lstlisting}
可以看到，我们仅仅是将核心的计算部分改为了cuda的核函数。将这份代码编译训练1个epoch后，得到如下结果：
\begin{lstlisting}
epoch:0 average_acc:0.856976 time used:71.9752s
\end{lstlisting}
可以看到，相比于cpu版本，速度提升了将近一倍。然而我们发现，cuda版本的速度并没有达到我们的预期，因此我们尝试进一步优化。使用NVIDIA的nsight进行性能分析，得到如下结果：

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{nsight1.jpg}
    \caption{cuda性能分析1}
\end{figure}

可以看到，cudaFree占了49.3\%的时间，cudaMalloc占了29.8\%的时间，cudaMemcpy占了8.9\%的时间，远远超过了cudaLaunchKernel的时间。因此我们尝试优化这几部分。
\subsection{优化内存分配}
观察代码可以发现，我们在每次调用矩阵相关函数时，都需要在栈空间上分配cuda指针，申请cuda内存，又在函数结束时调用析构函数释放cuda内存。这样的操作会导致频繁的cudaMalloc和cudaFree，从而导致性能下降。因此，我们尝试将每个中间结果以static的形式保存在全局变量中，避免重复的cudaMalloc和cudaFree。具体代码如下：
\begin{lstlisting}[language=C++]
void forward(const Mat &input, Mat &z1, Mat &a1,
             Mat &z2, Mat &a2) {
    static Mat input_mult_W1(input.n, W1.m);
    Mult_mat(input, W1, input_mult_W1);
    Add_mat(input_mult_W1, b1, z1);
    Relu(z1, a1);

    static Mat a1_mult_W2(a1.n, W2.m);
    Mult_mat(a1, W2, a1_mult_W2);
    Add_mat(a1_mult_W2, b2, z2);
    Softmax(z2, a2);
}
void Mult_mat(const Mat &a, const Mat &b, Mat &c) {
    assert(a.m == b.n);
    assert(a.n == c.n);
    assert(b.m == c.m);
    mult_mat_kernel<<<(a.n * b.m + 1023) / 1024, 1024>>>(
        a.a, b.a, c.a, a.n, a.m, b.m);
}
\end{lstlisting}
这样，我们优化掉了绝大部分的cudaMalloc和cudaFree，从而提升了性能。测试结果如下：
% 0 0.858262 18.8083
% 1 0.923024 19.0955 +
\begin{lstlisting}
epoch:0 average_acc:0.857357 time used:18.3029s
epoch:1 average_acc:0.923952 time used:36.164s
epoch:2 average_acc:0.940238 time used:53.9217s
epoch:3 average_acc:0.951690 time used:71.6025s
epoch:4 average_acc:0.958976 time used:89.4938s
epoch:5 average_acc:0.964762 time used:107.319s
epoch:6 average_acc:0.969524 time used:125.155s
epoch:7 average_acc:0.973071 time used:143.015s
epoch:8 average_acc:0.976024 time used:161.011s
epoch:9 average_acc:0.978238 time used:179.375s
\end{lstlisting}
可以看到，相较于cpu版本已经达到了约7倍的加速比。我们继续使用NVIDIA的nsight进行性能分析，得到如下结果：
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{nsight2.jpg}
    \caption{cuda性能分析2}
\end{figure}
可以看出，cudaMalloc和cudaFree的时间大大减少，而cudaMemcpy占了52.6\%的运行时间。因此我们尝试继续优化cudaMemcpy。
\subsection{优化cudaMemcpy}
使用nsight，我们能发现主要的cudaMemcpy操作都是在计算Accuracy和Loss时，将cuda内存中的数据拷贝到主机内存中。我们以计算Accuracy为例，介绍我们的优化方法。
\begin{lstlisting}[language=C++]
__global__ void Accuracy_kernel(double *a, double *b, int n,
                                int *c) {
    int Maxa = 0, Maxb = 0;
    for (int i = 0; i < n; i++) {
        if (a[i] > a[Maxa]) Maxa = i;
        if (b[i] > b[Maxb]) Maxb = i;
    }
    *c = Maxa == Maxb;
}

double Accuracy(const Mat &a, const Mat &b) {
    // a,b are 1*n matrix

    static int *c;
    cudaMalloc(&c, sizeof(int));
    Accuracy_kernel<<<1, 1>>>(a.a, b.a, a.m, c);
    int host_c;
    cudaMemcpy(&host_c, c, sizeof(int),
               cudaMemcpyDeviceToHost);

    return host_c;
}
\end{lstlisting}
我们可以看到，由于我们需要将一个int类型的数据拷贝到主机中，因此使用了cudaMemcpy。这样的操作会导致性能下降。因此，我们将所有的Accuracy和Loss的计算都放在cuda内存中进行，并在主程序中使用cuda数组来保存这些数据，直到需要计算Average Accuracy时，再将数据拷贝到主机内存中。这样，我们就避免了频繁的cudaMemcpy操作，从而提升了性能。输出结果如下：
\begin{lstlisting}
epoch:0 average_acc:0.856595 time used:13.6053s
epoch:1 average_acc:0.923310 time used:27.1446s
epoch:2 average_acc:0.940500 time used:40.6845s
epoch:3 average_acc:0.951762 time used:54.2125s
epoch:4 average_acc:0.959762 time used:67.7621s
epoch:5 average_acc:0.965143 time used:81.2944s
epoch:6 average_acc:0.969619 time used:94.8306s
epoch:7 average_acc:0.972905 time used:108.369s
epoch:8 average_acc:0.976071 time used:121.902s
epoch:9 average_acc:0.978238 time used:135.449s
\end{lstlisting}
可以看到，相较于cpu版本，我们的cuda版本已经达到了约9.5倍的加速比。运用nsight进行性能分析，得到如下结果：
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{nsight3.jpg}
    \caption{cuda性能分析3}
\end{figure}
可以看到，主要的时间开销都在cudaLaunchKernel上，占据了总运行时间的87.3\%。因此，接下来的优化方向应该是优化时间开销大的核函数，如矩阵乘法。不过由于时间限制，暂时没有进一步优化。我们将测试结果提交至kaggle上，得到如下结果：
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{gpu_result.jpg}
    \caption{并行版本的kaggle提交结果}
\end{figure}
可以看到我们的并行方法在达到9.5倍加速比的同时，准确率也有略微提升，达到了96.135\%。
\section{总结}
在本次实验中，我们使用了mnist数据集，训练了一个mlp自动感知机，并使用其预测了测试集，将结果提交至kaggle上的手写数字识别项目。分别实现了cpu串行版本和gpu并行版本，并对gpu并行版本进行优化，最终达到了96.1\%的准确率，以及相比于cpu版本的9.5倍的加速比。

通过本次实验，我学会了如何使用cuda进行并行计算，以及如何使用nsight调试并优化cuda程序。
% \ctexset{bibname=参考资料}
% \begin{thebibliography}{100}
%       \bibitem{ref1}\href{https://www.kaggle.com/code/ambrosm/pss3e20-eda-which-makes-sense}{PSS3E20 EDA which makes sense}
%       \bibitem{ref2}\href{https://www.kaggle.com/code/kacperrabczewski/rwanda-co2-step-by-step-guide}{Rwanda CO2: Step by step guide}
%       \bibitem{ref3}\href{https://www.kaggle.com/code/yaaangzhou/pg-s3-e20-eda-modeling}{[PG S3 E20] EDA + Modeling}
%       \bibitem{ref4}\href{https://www.kaggle.com/code/dmitryuarov/ps3e20-rwanda-emission-advanced-fe-20-88}{PS3E20 | Rwanda emission | Advanced FE | 20.88}
% \end{thebibliography}

\end{sloppypar}
\end{document}
